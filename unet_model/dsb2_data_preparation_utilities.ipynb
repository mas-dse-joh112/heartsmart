{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip show tensorflow\n",
    "# ! pip show tensorflow-gpu\n",
    "# !pip show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/masvol/heartsmart/unet_model\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully imported packages!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import cv2 \n",
    "import re, sys\n",
    "import fnmatch, shutil, subprocess\n",
    "from IPython.utils import io\n",
    "import glob\n",
    "\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, concatenate, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "#Fix the random seeds for numpy (this is for Keras) and for tensorflow backend to reduce the run-to-run variance\n",
    "from numpy.random import seed\n",
    "seed(100)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\nSuccessfully imported packages!!!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allocator_type = 'BFC'\n",
    "# #config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# #config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# #sess.run(yourcommand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.1  # train/test split ratio for Sunnybrook and ACDC data\n",
    "\n",
    "BASE_DIR = \"/opt/output/\"\n",
    "SOURCE = \"sunnybrook\"\n",
    "SB_SOURCE = \"sunnybrook\"\n",
    "ACDC_SOURCE = \"acdc\"\n",
    "TRAIN_IMG_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "TRAIN_LBL_DIR = BASE_DIR + SOURCE + \"/norm/1/3/labels/\"\n",
    "\n",
    "TEST_IMG_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "PRED_RESULT_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "\n",
    "#UNET_TRAIN_DIR = BASE_DIR + SOURCE + \"/unet_model/data/\"\n",
    "UNET_MODEL_DIR = BASE_DIR + SOURCE + \"/unet_model/models/\"\n",
    "\n",
    "\n",
    "UNET_TRAIN_DIR = \"/masvol/heartsmart/unet_model/data/\"\n",
    "UNET_MODEL_DIR = \"/masvol/heartsmart/unet_model/models/\"\n",
    "UNET_TRAIN_DIR2 = \"/masvol/output/acdc/norm/1/3/unet_model/data/\"\n",
    "\n",
    "# UNET_TRAIN_DIR = \"/opt/heartsmart/unet_model/data/\"\n",
    "# UNET_MODEL_DIR = \"/opt/heartsmart/unet_model/models/\"\n",
    "\n",
    "DSB_SOURCE = \"dsb\"\n",
    "DSB_TRAIN_IMG_DIR = BASE_DIR + DSB_SOURCE + \"/norm/1/3/train/\"\n",
    "DSB_VAL_IMG_DIR = BASE_DIR + DSB_SOURCE + \"/norm/1/3/validate/\"\n",
    "DSB_TEST_IMG_DIR = BASE_DIR + DSB_SOURCE + \"/norm/1/3/test/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to process and save DSB2 data set into 4d numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "#\n",
    "# Methods to process and save DSB2 data set into 4d numpy arrays\n",
    "#\n",
    "################################################\n",
    "def crop_center(img,cropx,cropy):\n",
    "    x,y = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[startx:startx+cropx, starty:starty+cropy]\n",
    "\n",
    "def pad_image(img, img_size):\n",
    "    #print (\"input shape : \", img.shape)\n",
    "    pad_x=0\n",
    "    pad_y=0\n",
    "    x,y = img.shape\n",
    "    if (x<img_size):\n",
    "        pad_x = img_size - x\n",
    "    if (y<img_size):\n",
    "        pad_y = img_size - y\n",
    "    process_img = np.pad(img, pad_width=((pad_x//2, ((pad_x//2) + (pad_x % 2))), (pad_y//2, ((pad_y//2) + (pad_y % 2)))), mode = 'constant', constant_values = 0)\n",
    "    #print (\"output shape : \", process_img.shape)\n",
    "    return process_img\n",
    "\n",
    "\n",
    "def shrink_case(case):\n",
    "    toks = case.split(\"-\")    \n",
    "    def shrink_if_number(x):\n",
    "        try:\n",
    "            cvt = int(x)\n",
    "            return str(cvt)\n",
    "        except ValueError:\n",
    "            return x\n",
    "    return \"-\".join([shrink_if_number(t) for t in toks])\n",
    "\n",
    "class Image_info_map(object):\n",
    "    def __init__(self, ctr_path):\n",
    "        self.ctr_path = ctr_path\n",
    "        #print (ctr_path)\n",
    "        #/opt/output/dsb/norm/1/3/test/1111/sax_13_IM-2680-0008.dcm.npy\n",
    "        match = re.search(r\"/([^/]*)/sax_(\\d+)_IM-(\\d+)-(\\d+).dcm.npy\", ctr_path)\n",
    "        #match = re.search(r\"/([^/]*)/patient\\d+_slice(\\d+)_frame(\\d+)_label_fix.nii.npy\", ctr_path)\n",
    "        try:\n",
    "            self.case = shrink_case(match.group(1))\n",
    "            self.sax_num = int (match.group(2))\n",
    "            self.record = int(match.group(3))\n",
    "            self.img_no = int(match.group(4))\n",
    "        except AttributeError:\n",
    "            #/opt/output/dsb/norm/1/3/train/234/sax_20_IM-3098-0022-0007.dcm.npy\n",
    "            match = re.search(r\"/([^/]*)/sax_(\\d+)_IM-(\\d+)-(\\d+)-(\\d+).dcm.npy\", ctr_path)\n",
    "            self.case = shrink_case(match.group(1))\n",
    "            self.sax_num = int (match.group(2))\n",
    "            self.record = int(match.group(3))\n",
    "            self.img_no = int(match.group(5))\n",
    "            \n",
    "    def __str__(self):\n",
    "        return \"<Image info for case %s, record %d image %d>\" % (self.case, self.record, self.img_no)\n",
    "    \n",
    "def get_dsb_image_list2(data_path):\n",
    "    image_list = [os.path.join(dirpath,f) \n",
    "            for files in glob.glob(data_path+\"/*\") \n",
    "            for dirpath, dirname, infiles in os.walk(files) \n",
    "            for f in infiles if f.endswith('dcm.npy')]\n",
    "    \n",
    "    print(\"Number of examples: {:d}\".format(len(image_list)))\n",
    "    #print(\"Shuffle data\")\n",
    "    #np.random.shuffle(image_list)\n",
    "    print (image_list[0], image_list[-1])\n",
    "    \n",
    "    extracted = list(map(Image_info_map, image_list))\n",
    "    print (\"Image 0 :\", extracted[0].case, extracted[0].record, extracted[0].img_no)\n",
    "    print (\"Imae -1 :\", extracted[-1].case, extracted[-1].record, extracted[-1].img_no) \n",
    "    return image_list, extracted\n",
    "\n",
    "def get_dsb_image_list(data_path):\n",
    "    image_list = [os.path.join(data_path,files) \n",
    "            for files in glob.glob(data_path+\"/*\") if files.endswith('dcm.npy')]\n",
    "    \n",
    "    print(\"Number of examples: {:d}\".format(len(image_list)))\n",
    "    #print(\"Shuffle data\")\n",
    "    #np.random.shuffle(image_list)\n",
    "    print (image_list[0], image_list[-1])\n",
    "    \n",
    "    extracted = list(map(Image_info_map, image_list))\n",
    "    print (\"Image 0 :\", extracted[0].case, extracted[0].record, extracted[0].img_no)\n",
    "    print (\"Imae -1 :\", extracted[-1].case, extracted[-1].record, extracted[-1].img_no) \n",
    "    return image_list, extracted\n",
    "\n",
    "def get_dsb_images(data_path, crop_size):\n",
    "    print (\"data path: \", data_path)\n",
    "    img_path_list, extracted_info = get_dsb_image_list(data_path)\n",
    "    img_count = len(img_path_list)\n",
    "    print(\"Processing {:d} images and labels...\".format(img_count))\n",
    "    \n",
    "    imgs , img_path, ext_info = [], [], []\n",
    "    for i in range(img_count):\n",
    "\n",
    "        full_path = img_path_list[i]\n",
    "        img = np.load(full_path)\n",
    "        x,y = img.shape\n",
    "\n",
    "        if x < crop_size or y < crop_size:\n",
    "            #print (\"shapes smaller than crop size\", x, y, crop_size)\n",
    "            img = pad_image(img, crop_size)\n",
    "            #continue\n",
    "        if x > crop_size or y > crop_size:\n",
    "            #print (\"img: \", i, x, y, full_path)\n",
    "            img = crop_center(img,crop_size,crop_size)\n",
    "            \n",
    "        imgs.append(img)\n",
    "        img_path.append(full_path)\n",
    "        ext_info.append(extracted_info[i])\n",
    "\n",
    "        if i % (img_count//5) == 0:\n",
    "            print (full_path)\n",
    "            plt.imshow(img, cmap = 'gray')\n",
    "            plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
    "            plt.show()\n",
    "\n",
    "            #except IOError:\n",
    "            #    continue\n",
    "    print (\"Final size of image set :\", len(imgs), \"dropped images:\", (img_count - len(imgs)))\n",
    "                \n",
    "    return imgs, img_path, ext_info\n",
    "\n",
    "def convert_images_to_nparray_and_save (imgs, save_file, image_size):\n",
    "    rows = image_size\n",
    "    cols = image_size\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print(\"Converting data to np array, Input size : \",len(imgs))\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgdatas = np.ndarray((len(imgs),rows,cols,1), dtype=np.int)\n",
    "        \n",
    "    for idx in range(len(imgs)):\n",
    "        img = imgs[idx]\n",
    "        img = img_to_array(img)        \n",
    "        try:\n",
    "            imgdatas[i] = img\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        \n",
    "    np.save(save_file, imgdatas)\n",
    "\n",
    "    print (\"Shape of image array : \", imgdatas.shape)\n",
    "    print (\"Max, min, mean values\", imgdatas.max(), imgdatas.min(), imgdatas.mean())\n",
    "    print('Saved data as: ', save_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patient_record = \"204\"\n",
    "data_path = DSB_TRAIN_IMG_DIR + patient_record\n",
    "image_list, image_path_list, extracted_info = get_dsb_images(data_path, crop_size=176)\n",
    "image_list2, image_path_list2, extracted_info2 = get_dsb_images(data_path, crop_size=256)\n",
    "\n",
    "img_file = UNET_TRAIN_DIR + DSB_SOURCE + patient_record + \"_176.npy\"\n",
    "convert_images_to_nparray_and_save (image_list, img_file, image_size = 176)\n",
    "\n",
    "img_file2 = UNET_TRAIN_DIR + DSB_SOURCE + patient_record + \"_256.npy\"\n",
    "convert_images_to_nparray_and_save (image_list2, img_file2, image_size = 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to load 4d np array for images from ./data directory, perform pixel normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#\n",
    "# Methods to load 4d np array for images from ./data directory\n",
    "# 4D tensor with shape: (samples, rows, cols, channels=1)\n",
    "#\n",
    "#########################################\n",
    "\n",
    "def load_images_and_labels(data):\n",
    "    print('-'*30)\n",
    "    print('load np arrays of images and labels...')\n",
    "    print('-'*30)\n",
    "    imgfile = data[\"images\"]\n",
    "    labelfile = data[\"labels\"]\n",
    "    print (\"Loading files : \", imgfile, labelfile)\n",
    "    \n",
    "    im = np.load(imgfile)\n",
    "    lb = np.load(labelfile)\n",
    "    images = im.astype('float32')\n",
    "    labels = lb.astype('float32')\n",
    "    \n",
    "    ##Normalize the pixel values, (between 0..1)\n",
    "    x_min = images.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = images.max(axis=(1, 2), keepdims=True)\n",
    "    images2 = (images - x_min)/(x_max-x_min)\n",
    "\n",
    "    print(\"shape, max, min, mean of original image set:\", images.shape, images.max(), images.min(), images.mean())\n",
    "    print(\"shape, max, min, mean after normalization  :\", images2.shape, images2.max(), images2.min(), images2.mean())\n",
    "    print(\"shape, max, min, mean of labels :\", labels.shape, labels.max(), labels.min(), labels.mean())\n",
    "    return images2, labels\n",
    "\n",
    "\n",
    "def load_images(imgfile):\n",
    "    print('-'*30)\n",
    "    print('load np arrays of images ...')\n",
    "    print('-'*30)\n",
    "    print (\"Loading files : \", imgfile)\n",
    "    \n",
    "    im = np.load(imgfile)\n",
    "    images = im.astype('float32')\n",
    "    \n",
    "    ##Normalize the pixel values, (between 0..1)\n",
    "    x_min = images.min(axis=(1, 2), keepdims=True)\n",
    "    x_max = images.max(axis=(1, 2), keepdims=True)\n",
    "    images2 = (images - x_min)/(x_max-x_min)\n",
    "\n",
    "    print(\"shape, max, min, mean of original image set:\", images.shape, images.max(), images.min(), images.mean())\n",
    "    print(\"shape, max, min, mean after normalization  :\", images2.shape, images2.max(), images2.min(), images2.mean())\n",
    "    return images2\n",
    "\n",
    "def combine_acdc_sunnybrook_data_176():\n",
    "    acdc_train_data = {}\n",
    "    acdc_train_data[\"images\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_176_train_images.npy\"\n",
    "    acdc_train_data[\"labels\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_176_train_labels.npy\"\n",
    "\n",
    "    acdc_train_img, acdc_train_lbl = load_images_and_labels(acdc_train_data)\n",
    "\n",
    "    acdc_test_data = {}\n",
    "    acdc_test_data[\"images\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_176_test_images.npy\"\n",
    "    acdc_test_data[\"labels\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_176_test_labels.npy\"\n",
    "\n",
    "    acdc_test_img, acdc_test_lbl = load_images_and_labels(acdc_test_data)\n",
    "\n",
    "    sb_train_data = {}\n",
    "    sb_train_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_176_train_images.npy\"\n",
    "    sb_train_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_176_train_labels.npy\"\n",
    "    sb_train_img, sb_train_lbl = load_images_and_labels(sb_train_data)\n",
    "\n",
    "    sb_test_data = {}\n",
    "    sb_test_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_176_test_images.npy\"\n",
    "    sb_test_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_176_test_labels.npy\"\n",
    "    sb_test_img, sb_test_lbl = load_images_and_labels(sb_test_data)\n",
    "\n",
    "    combined_train_img = np.concatenate((acdc_train_img, sb_train_img), axis=0)\n",
    "    combined_train_lbl = np.concatenate((acdc_train_lbl, sb_train_lbl), axis=0)\n",
    "\n",
    "    combined_test_img = np.concatenate((acdc_test_img, sb_test_img), axis=0)\n",
    "    combined_test_lbl = np.concatenate((acdc_test_lbl, sb_test_lbl), axis=0)\n",
    "    print (combined_train_img.shape, combined_train_lbl.shape,combined_test_img.shape,combined_test_lbl.shape)\n",
    "    print (\"Saving combined files.......\")\n",
    "\n",
    "    tr_img_file = UNET_TRAIN_DIR + \"combined_176_train_images.npy\"\n",
    "    tr_lbl_file = UNET_TRAIN_DIR + \"combined_176_train_labels.npy\"\n",
    "    tst_img_file = UNET_TRAIN_DIR + \"combined_176_test_images.npy\"\n",
    "    tst_lbl_file = UNET_TRAIN_DIR + \"combined_176_test_labels.npy\"\n",
    "\n",
    "    np.save(tr_img_file, combined_train_img)\n",
    "    np.save(tr_lbl_file, combined_train_lbl)\n",
    "    np.save(tst_img_file, combined_test_img)\n",
    "    np.save(tst_lbl_file, combined_test_lbl)\n",
    "    \n",
    "def combine_acdc_sunnybrook_data_256():\n",
    "    acdc_train_data = {}\n",
    "    acdc_train_data[\"images\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_256_train_images.npy\"\n",
    "    acdc_train_data[\"labels\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_256_train_labels.npy\"\n",
    "\n",
    "    acdc_train_img, acdc_train_lbl = load_images_and_labels(acdc_train_data)\n",
    "\n",
    "    acdc_test_data = {}\n",
    "    acdc_test_data[\"images\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_256_test_images.npy\"\n",
    "    acdc_test_data[\"labels\"] = UNET_TRAIN_DIR + ACDC_SOURCE + \"_256_test_labels.npy\"\n",
    "\n",
    "    acdc_test_img, acdc_test_lbl = load_images_and_labels(acdc_test_data)\n",
    "\n",
    "    sb_train_data = {}\n",
    "    sb_train_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_256_train_images.npy\"\n",
    "    sb_train_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_256_train_labels.npy\"\n",
    "    sb_train_img, sb_train_lbl = load_images_and_labels(sb_train_data)\n",
    "\n",
    "    sb_test_data = {}\n",
    "    sb_test_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_256_test_images.npy\"\n",
    "    sb_test_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_256_test_labels.npy\"\n",
    "    sb_test_img, sb_test_lbl = load_images_and_labels(sb_test_data)\n",
    "\n",
    "    combined_train_img = np.concatenate((acdc_train_img, sb_train_img), axis=0)\n",
    "    combined_train_lbl = np.concatenate((acdc_train_lbl, sb_train_lbl), axis=0)\n",
    "\n",
    "    combined_test_img = np.concatenate((acdc_test_img, sb_test_img), axis=0)\n",
    "    combined_test_lbl = np.concatenate((acdc_test_lbl, sb_test_lbl), axis=0)\n",
    "    print (combined_train_img.shape, combined_train_lbl.shape,combined_test_img.shape,combined_test_lbl.shape)\n",
    "    print (\"Saving combined files.......\")\n",
    "\n",
    "    tr_img_file = UNET_TRAIN_DIR + \"combined_256_train_images.npy\"\n",
    "    tr_lbl_file = UNET_TRAIN_DIR + \"combined_256_train_labels.npy\"\n",
    "    tst_img_file = UNET_TRAIN_DIR + \"combined_256_test_images.npy\"\n",
    "    tst_lbl_file = UNET_TRAIN_DIR + \"combined_256_test_labels.npy\"\n",
    "\n",
    "    np.save(tr_img_file, combined_train_img)\n",
    "    np.save(tr_lbl_file, combined_train_lbl)\n",
    "    np.save(tst_img_file, combined_test_img)\n",
    "    np.save(tst_lbl_file, combined_test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_acdc_sunnybrook_data_176()\n",
    "#combine_acdc_sunnybrook_data_256()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data_176():\n",
    "    acdc_train_data = {}\n",
    "    acdc_train_data[\"images\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_176_train_orig_images.npy\"\n",
    "    acdc_train_data[\"labels\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_176_train_orig_labels.npy\"\n",
    "\n",
    "    acdc_train_img, acdc_train_lbl = load_images_and_labels(acdc_train_data)\n",
    "\n",
    "    acdc_test_data = {}\n",
    "    acdc_test_data[\"images\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_176_test_orig_images.npy\"\n",
    "    acdc_test_data[\"labels\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_176_test_orig_labels.npy\"\n",
    "\n",
    "    acdc_test_img, acdc_test_lbl = load_images_and_labels(acdc_test_data)\n",
    "\n",
    "    sb_train_data = {}\n",
    "    sb_train_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_176_train_images.npy\"\n",
    "    sb_train_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_176_train_labels.npy\"\n",
    "    sb_train_img, sb_train_lbl = load_images_and_labels(sb_train_data)\n",
    "\n",
    "    sb_test_data = {}\n",
    "    sb_test_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_176_test_images.npy\"\n",
    "    sb_test_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_176_test_labels.npy\"\n",
    "    sb_test_img, sb_test_lbl = load_images_and_labels(sb_test_data)\n",
    "\n",
    "    combined_train_img = np.concatenate((acdc_train_img, sb_train_img), axis=0)\n",
    "    combined_train_lbl = np.concatenate((acdc_train_lbl, sb_train_lbl), axis=0)\n",
    "\n",
    "    combined_test_img = np.concatenate((acdc_test_img, sb_test_img), axis=0)\n",
    "    combined_test_lbl = np.concatenate((acdc_test_lbl, sb_test_lbl), axis=0)\n",
    "    print (combined_train_img.shape, combined_train_lbl.shape,combined_test_img.shape,combined_test_lbl.shape)\n",
    "    print (\"Saving combined files.......\")\n",
    "\n",
    "    tr_img_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_176_train_images.npy\"\n",
    "    tr_lbl_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_176_train_labels.npy\"\n",
    "    tst_img_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_176_test_images.npy\"\n",
    "    tst_lbl_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_176_test_labels.npy\"\n",
    "\n",
    "    np.save(tr_img_file, combined_train_img)\n",
    "    np.save(tr_lbl_file, combined_train_lbl)\n",
    "    np.save(tst_img_file, combined_test_img)\n",
    "    np.save(tst_lbl_file, combined_test_lbl)\n",
    "    \n",
    "def combine_data_256():\n",
    "    acdc_train_data = {}\n",
    "    acdc_train_data[\"images\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_256_train_orig_images.npy\"\n",
    "    acdc_train_data[\"labels\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_256_train_orig_labels.npy\"\n",
    "\n",
    "    acdc_train_img, acdc_train_lbl = load_images_and_labels(acdc_train_data)\n",
    "\n",
    "    acdc_test_data = {}\n",
    "    acdc_test_data[\"images\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_256_test_orig_images.npy\"\n",
    "    acdc_test_data[\"labels\"] = UNET_TRAIN_DIR2 + ACDC_SOURCE + \"_256_test_orig_labels.npy\"\n",
    "\n",
    "    acdc_test_img, acdc_test_lbl = load_images_and_labels(acdc_test_data)\n",
    "\n",
    "    sb_train_data = {}\n",
    "    sb_train_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_256_train_images.npy\"\n",
    "    sb_train_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_256_train_labels.npy\"\n",
    "    sb_train_img, sb_train_lbl = load_images_and_labels(sb_train_data)\n",
    "\n",
    "    sb_test_data = {}\n",
    "    sb_test_data[\"images\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_256_test_images.npy\"\n",
    "    sb_test_data[\"labels\"] = UNET_TRAIN_DIR + SB_SOURCE + \"_1_3_256_test_labels.npy\"\n",
    "    sb_test_img, sb_test_lbl = load_images_and_labels(sb_test_data)\n",
    "\n",
    "    combined_train_img = np.concatenate((acdc_train_img, sb_train_img), axis=0)\n",
    "    combined_train_lbl = np.concatenate((acdc_train_lbl, sb_train_lbl), axis=0)\n",
    "\n",
    "    combined_test_img = np.concatenate((acdc_test_img, sb_test_img), axis=0)\n",
    "    combined_test_lbl = np.concatenate((acdc_test_lbl, sb_test_lbl), axis=0)\n",
    "    print (combined_train_img.shape, combined_train_lbl.shape,combined_test_img.shape,combined_test_lbl.shape)\n",
    "    print (\"Saving combined files.......\")\n",
    "\n",
    "    tr_img_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_256_train_images.npy\"\n",
    "    tr_lbl_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_256_train_labels.npy\"\n",
    "    tst_img_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_256_test_images.npy\"\n",
    "    tst_lbl_file = UNET_TRAIN_DIR2 + \"combined_1_3_0_256_test_labels.npy\"\n",
    "\n",
    "    np.save(tr_img_file, combined_train_img)\n",
    "    np.save(tr_lbl_file, combined_train_lbl)\n",
    "    np.save(tst_img_file, combined_test_img)\n",
    "    np.save(tst_lbl_file, combined_test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/output/acdc/norm/1/3/unet_model/data/acdc_256_train_orig_images.npy /masvol/output/acdc/norm/1/3/unet_model/data/acdc_256_train_orig_labels.npy\n",
      "shape, max, min, mean of original image set: (1712, 256, 256, 1) 3477.0 0.0 76.44866\n",
      "shape, max, min, mean after normalization  : (1712, 256, 256, 1) 1.0 0.0 0.20411113\n",
      "shape, max, min, mean of labels : (1712, 256, 256, 1) 1.0 0.0 0.021720342\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/output/acdc/norm/1/3/unet_model/data/acdc_256_test_orig_images.npy /masvol/output/acdc/norm/1/3/unet_model/data/acdc_256_test_orig_labels.npy\n",
      "shape, max, min, mean of original image set: (190, 256, 256, 1) 2397.0 0.0 75.72695\n",
      "shape, max, min, mean after normalization  : (190, 256, 256, 1) 1.0 0.0 0.2126041\n",
      "shape, max, min, mean of labels : (190, 256, 256, 1) 1.0 0.0 0.022837428\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/heartsmart/unet_model/data/sunnybrook_1_3_256_train_images.npy /masvol/heartsmart/unet_model/data/sunnybrook_1_3_256_train_labels.npy\n",
      "shape, max, min, mean of original image set: (725, 256, 256, 1) 3005.0 0.0 110.46942\n",
      "shape, max, min, mean after normalization  : (725, 256, 256, 1) 1.0 0.0 0.12546965\n",
      "shape, max, min, mean of labels : (725, 256, 256, 1) 1.0 0.0 0.030884799\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/heartsmart/unet_model/data/sunnybrook_1_3_256_test_images.npy /masvol/heartsmart/unet_model/data/sunnybrook_1_3_256_test_labels.npy\n",
      "shape, max, min, mean of original image set: (80, 256, 256, 1) 3071.0 0.0 118.60573\n",
      "shape, max, min, mean after normalization  : (80, 256, 256, 1) 1.0 0.0 0.12709329\n",
      "shape, max, min, mean of labels : (80, 256, 256, 1) 1.0 0.0 0.03285408\n",
      "(2437, 256, 256, 1) (2437, 256, 256, 1) (270, 256, 256, 1) (270, 256, 256, 1)\n",
      "Saving combined files.......\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/output/acdc/norm/1/3/unet_model/data/acdc_176_train_orig_images.npy /masvol/output/acdc/norm/1/3/unet_model/data/acdc_176_train_orig_labels.npy\n",
      "shape, max, min, mean of original image set: (1712, 176, 176, 1) 3477.0 0.0 80.78445\n",
      "shape, max, min, mean after normalization  : (1712, 176, 176, 1) 1.0 0.0 0.23022509\n",
      "shape, max, min, mean of labels : (1712, 176, 176, 1) 1.0 0.0 0.045931004\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/output/acdc/norm/1/3/unet_model/data/acdc_176_test_orig_images.npy /masvol/output/acdc/norm/1/3/unet_model/data/acdc_176_test_orig_labels.npy\n",
      "shape, max, min, mean of original image set: (190, 176, 176, 1) 1797.0 0.0 81.69327\n",
      "shape, max, min, mean after normalization  : (190, 176, 176, 1) 1.0 0.0 0.2230458\n",
      "shape, max, min, mean of labels : (190, 176, 176, 1) 1.0 0.0 0.04821101\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/heartsmart/unet_model/data/sunnybrook_1_3_176_train_images.npy /masvol/heartsmart/unet_model/data/sunnybrook_1_3_176_train_labels.npy\n",
      "shape, max, min, mean of original image set: (725, 176, 176, 1) 2322.0 0.0 112.19226\n",
      "shape, max, min, mean after normalization  : (725, 176, 176, 1) 1.0 0.0 0.16628249\n",
      "shape, max, min, mean of labels : (725, 176, 176, 1) 1.0 0.0 0.066138946\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /masvol/heartsmart/unet_model/data/sunnybrook_1_3_176_test_images.npy /masvol/heartsmart/unet_model/data/sunnybrook_1_3_176_test_labels.npy\n",
      "shape, max, min, mean of original image set: (80, 176, 176, 1) 2479.0 0.0 113.15008\n",
      "shape, max, min, mean after normalization  : (80, 176, 176, 1) 1.0 0.0 0.16859826\n",
      "shape, max, min, mean of labels : (80, 176, 176, 1) 1.0 0.0 0.062296618\n",
      "(2437, 176, 176, 1) (2437, 176, 176, 1) (270, 176, 176, 1) (270, 176, 176, 1)\n",
      "Saving combined files.......\n"
     ]
    }
   ],
   "source": [
    "combine_data_256()\n",
    "combine_data_176()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
