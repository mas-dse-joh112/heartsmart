{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow-gpu\n",
      "Version: 1.6.0\n",
      "Summary: TensorFlow helps the tensors flow\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.5/dist-packages\n",
      "Requires: gast, absl-py, termcolor, numpy, tensorboard, astor, wheel, protobuf, grpcio, six\n",
      "Required-by: \n",
      "Name: Keras\n",
      "Version: 2.1.5\n",
      "Summary: Deep Learning for humans\n",
      "Home-page: https://github.com/keras-team/keras\n",
      "Author: Francois Chollet\n",
      "Author-email: francois.chollet@gmail.com\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.5/dist-packages\n",
      "Requires: pyyaml, numpy, scipy, six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow\n",
    "\n",
    "!pip show tensorflow-gpu\n",
    "\n",
    "!pip show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True, device_count = {'GPU':3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto(\n",
    "#        device_count = {'GPU': 0}\n",
    "#    )\n",
    "#sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully imported packages!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import cv2 \n",
    "import re, sys\n",
    "import fnmatch, shutil, subprocess\n",
    "from IPython.utils import io\n",
    "import glob\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, concatenate, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "#Fix the random seeds for numpy (this is for Keras) and for tensorflow backend to reduce the run-to-run variance\n",
    "from numpy.random import seed\n",
    "seed(100)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\nSuccessfully imported packages!!!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils.training_utils import multi_gpu_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "IMAGE_SIZE = 256\n",
    "BASE_DIR = \"/opt/output/\"\n",
    "SOURCE = \"acdc\"\n",
    "TRAIN_IMG_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "TRAIN_LBL_DIR = BASE_DIR + SOURCE + \"/norm/1/3/labels/\"\n",
    "\n",
    "TEST_IMG_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "PRED_RESULT_DIR = BASE_DIR + SOURCE + \"/norm/1/3/images/\"\n",
    "\n",
    "UNET_TRAIN_DIR = BASE_DIR + SOURCE + \"/unet_model/data/\"\n",
    "UNET_MODEL_DIR = BASE_DIR + SOURCE + \"/unet_model/models/\"\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.1  # train/test split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Methods to extract acdc contour files and corresponding image files \n",
    "#\n",
    "###################################\n",
    "\n",
    "\n",
    "def shrink_case(case):\n",
    "    toks = case.split(\"-\")\n",
    "    \n",
    "    def shrink_if_number(x):\n",
    "        try:\n",
    "            cvt = int(x)\n",
    "            return str(cvt)\n",
    "        except ValueError:\n",
    "            return x\n",
    "\n",
    "    return \"-\".join([shrink_if_number(t) for t in toks])\n",
    "\n",
    "class Contour(object):\n",
    "    def __init__(self, ctr_path):\n",
    "        self.ctr_path = ctr_path\n",
    "        #print (ctr_path)\n",
    "        match = re.search(r\"/([^/]*)/patient\\d+_slice(\\d+)_frame(\\d+)_label_fix.nii.npy\", ctr_path)\n",
    "        self.case = shrink_case(match.group(1))\n",
    "        self.record = int(match.group(2))\n",
    "        self.img_no = int(match.group(3))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Contour for case %s, record %d image %d>\" % (self.case, self.record, self.img_no)\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "def load_contour(contour, img_path, crop_size):\n",
    "    # file = IM-0851-0127.dcm.npy\n",
    "    filename = \"%s_slice%s_frame%d.nii.npy\" % (contour.case,contour.record,contour.img_no)\n",
    "    full_path = os.path.join(img_path, contour.case, filename)\n",
    "    img = np.load(full_path)\n",
    "    label = np.load(contour.ctr_path)\n",
    "    height, width = img.shape\n",
    "    height_l, width_l = label.shape\n",
    "    \n",
    "    if height != crop_size or width != crop_size:\n",
    "        #print (\"img: \", contour.img_no, height, width)\n",
    "        #print (\"lbl: \", height_l, width_l)\n",
    "        img = crop_center(img,crop_size,crop_size)\n",
    "        label = crop_center(label,crop_size,crop_size)\n",
    "        \n",
    "    return img, label, full_path\n",
    "   \n",
    "def get_all_contours(contour_path):\n",
    "    contours = [os.path.join(dirpath,f) \n",
    "            for files in glob.glob(TRAIN_LBL_DIR+\"/*\") \n",
    "            for dirpath, dirname, infiles in os.walk(files) \n",
    "            for f in infiles if f.endswith('label_fix.nii.npy')]\n",
    "    \n",
    "    print(\"Number of examples: {:d}\".format(len(contours)))\n",
    "    print(\"Shuffle data\")\n",
    "    \n",
    "    np.random.shuffle(contours)\n",
    "    print (contours[0], contours[-1])\n",
    "    print(\"Number of examples after cleanup: {:d}\".format(len(contours)))\n",
    "    \n",
    "    extracted = list(map(Contour, contours))\n",
    "    print (\"Contour 0 :\", extracted[0].case, extracted[0].record, extracted[0].img_no)\n",
    "    print (\"Contour -1 :\", extracted[-1].case, extracted[-1].record, extracted[-1].img_no) \n",
    "    return extracted\n",
    "\n",
    "def get_contours_and_images(contours, img_path, crop_size):\n",
    "    counter_img = 0\n",
    "    counter_label = 0\n",
    "    batchsz = len(contours)\n",
    "    print(\"Processing {:d} images and labels...\".format(len(contours)))\n",
    "    \n",
    "    for i in range(int(np.ceil(len(contours) / float(batchsz)))):\n",
    "        batch = contours[(batchsz*i):(batchsz*(i+1))]\n",
    "        if len(batch) == 0:\n",
    "            break\n",
    "            \n",
    "        imgs, labels = [], []\n",
    "        imgs2, labels2 = [], []\n",
    "        \n",
    "        for idx,ctr in enumerate(batch):\n",
    "            #try:\n",
    "                filename = \"%s_slice%s_frame%d.nii.npy\" % (ctr.case,ctr.record,ctr.img_no)\n",
    "                full_path = os.path.join(img_path, ctr.case, filename)\n",
    "                img = np.load(full_path)\n",
    "                x,y = img.shape\n",
    "                    \n",
    "                if x < crop_size or y < crop_size:\n",
    "                    continue\n",
    "                \n",
    "                img, label, fullpath = load_contour(ctr, img_path, crop_size)\n",
    "                imgs.append(img)\n",
    "                labels.append(label)\n",
    "\n",
    "                if idx % 100 == 0:\n",
    "                    print (fullpath)\n",
    "                    f, axs = plt.subplots(1,3,figsize=(12,12))\n",
    "                    plt.subplot(131),plt.imshow(img, cmap = 'gray')\n",
    "                    plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
    "                    plt.subplot(132),plt.imshow(label, cmap = 'gray')\n",
    "                    plt.title('label'), plt.xticks([]), plt.yticks([])\n",
    "                    plt.subplot(133),plt.imshow(img, cmap = 'gray')\n",
    "                    plt.imshow(label, 'jet', interpolation='none', alpha=0.5)\n",
    "                    plt.title('label'), plt.xticks([]), plt.yticks([])\n",
    "                    plt.show()\n",
    "\n",
    "            #except IOError:\n",
    "            #    continue\n",
    "                \n",
    "    return imgs, labels\n",
    "\n",
    "def extract_acdc_training_data(crop_size=256): \n",
    "    SPLIT_RATIO = TRAIN_TEST_SPLIT_RATIO  # train/test split ratio\n",
    "    print(\"Mapping ground truth contours to images...\")\n",
    "    \n",
    "    ctrs = get_all_contours(TRAIN_LBL_DIR)\n",
    "    print(\"Done mapping ground truth contours to images\")\n",
    "    \n",
    "    test_ctrs = ctrs[0:int(SPLIT_RATIO*len(ctrs))]\n",
    "    train_ctrs = ctrs[int(SPLIT_RATIO*len(ctrs)):]\n",
    "    print(\"Split train_set:%d, test_set:%d\"%(len(train_ctrs), len(test_ctrs)))\n",
    "    print (\"Extracting Training Images and Labels\")\n",
    "    \n",
    "    train_imgs, train_labels = get_contours_and_images(train_ctrs, TRAIN_IMG_DIR, crop_size)\n",
    "    print (\"Extracting Test Images and Labels\")\n",
    "    \n",
    "    test_imgs, test_labels = get_contours_and_images(test_ctrs, TRAIN_IMG_DIR, crop_size)\n",
    "    print(\"Extracted Images train_set:%d, test_set:%d\"%(len(train_imgs), len(test_imgs)))\n",
    "    \n",
    "    return train_imgs, train_labels, test_imgs, test_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Get acdc images and labels with crop from center to get 256x256 images\n",
    "train_imgs, train_labels, test_imgs, test_labels = extract_acdc_training_data(crop_size=256)\n",
    "\n",
    "\n",
    "##Get acdc images and labels with crop from center to get 180x180 images\n",
    "\n",
    "train_imgs2, train_labels2, test_imgs2, test_labels2 = extract_acdc_training_data(crop_size=176)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#\n",
    "# Method to create  4d np array for images and labels and store the array in a directory\n",
    "# 4D tensor with shape: (samples, rows, cols, channels=1)\n",
    "#\n",
    "#########################################\n",
    "\n",
    "def create_training_data(imgs, lbls, save_file_path, file_prefix, image_size):\n",
    "    rows = image_size\n",
    "    cols = image_size\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print(\"Creating training data..input size : \",len(imgs))\n",
    "    print('-'*30)\n",
    "    print(\"Converting data to np array\")\n",
    "    \n",
    "    imgdatas = np.ndarray((len(imgs),rows,cols,1), dtype=np.int)\n",
    "    \n",
    "    imglabels = np.ndarray((len(imgs),rows,cols,1), dtype=np.uint8)\n",
    "    \n",
    "    for idx in range(len(imgs)):\n",
    "        img = imgs[idx]\n",
    "        label = lbls[idx]\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        \n",
    "        #x,y,z = img.shape\n",
    "        \n",
    "        #if x != 256 or y != 256:\n",
    "        #    continue\n",
    "        \n",
    "        try:\n",
    "            imgdatas[i] = img\n",
    "            imglabels[i] = label\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        \n",
    "    imgfile = save_file_path + file_prefix +\"_images.npy\"\n",
    "    print (imgfile)\n",
    "    lblfile = save_file_path + file_prefix +\"_labels.npy\"\n",
    "    np.save(imgfile, imgdatas)\n",
    "    np.save(lblfile, imglabels)\n",
    "\n",
    "    print (\"Shape of data & label np arrays : \", imgdatas.shape, imglabels.shape)\n",
    "    print (imgdatas.max(), imgdatas.min(), imglabels.max(), imglabels.min())\n",
    "    print('Saved data as: ', imgfile, lblfile )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create 256x256 size train/test data in 4d tensor shape and save them\n",
    "save_location = UNET_TRAIN_DIR\n",
    "tr_file_prefix = SOURCE + \"_256_train_orig\"\n",
    "tst_file_prefix = SOURCE + \"_256_test_orig\"\n",
    "create_training_data(train_imgs, train_labels, save_location, tr_file_prefix, image_size = 256)\n",
    "create_training_data(test_imgs, test_labels, save_location, tst_file_prefix, image_size = 256)\n",
    "\n",
    "### Create 180x180 size train/test data in 4d tensor shape and save them\n",
    "tr_file_prefix = SOURCE + \"_176_train_orig\"\n",
    "tst_file_prefix = SOURCE + \"_176_test_orig\"\n",
    "\n",
    "create_training_data(train_imgs2, train_labels2, save_location, tr_file_prefix, image_size = 176)\n",
    "create_training_data(test_imgs2, test_labels2, save_location, tst_file_prefix, image_size = 176)\n",
    "\n",
    "#create_test_data(test_imgs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from here  if images and labels are already stored in unet_model/data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#\n",
    "# Methods to load 4d np array for images from ./data directory\n",
    "# 4D tensor with shape: (samples, rows, cols, channels=1)\n",
    "#\n",
    "#########################################\n",
    "\n",
    "MAX_PIXEL_VAL = 0\n",
    "\n",
    "\n",
    "def load_images_and_labels(data):\n",
    "    print('-'*30)\n",
    "    print('load np arrays of images and labels...')\n",
    "    print('-'*30)\n",
    "    imgfile = data[\"images\"]\n",
    "    labelfile = data[\"labels\"]\n",
    "    print (\"Loading files : \", imgfile, labelfile)\n",
    "    \n",
    "    im = np.load(imgfile)\n",
    "    lb = np.load(labelfile)\n",
    "    images = im.astype('float32')\n",
    "    labels = lb.astype('float32')\n",
    "    \n",
    "    print (\"max : \", images.max())\n",
    "    ##Normalize the pixel values, (between 0..1)\n",
    "    MAX_PIXEL_VAL = images.max()\n",
    "    images /= MAX_PIXEL_VAL\n",
    "    \n",
    "    #mean = imgs_train.mean(axis = 0)\n",
    "    #imgs_train -= mean\t\n",
    "    print(images.shape, images.max(), images.min())\n",
    "    print(labels.shape, labels.max(), labels.min())\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /opt/output/acdc/unet_model/data/acdc_176_train_orig_images.npy /opt/output/acdc/unet_model/data/acdc_176_train_orig_labels.npy\n",
      "max :  2952.0\n",
      "(1657, 176, 176, 1) 1.0 0.0\n",
      "(1657, 176, 176, 1) 1.0 0.0\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /opt/output/acdc/unet_model/data/acdc_176_test_orig_images.npy /opt/output/acdc/unet_model/data/acdc_176_test_orig_labels.npy\n",
      "max :  1814.0\n",
      "(184, 176, 176, 1) 1.0 0.0\n",
      "(184, 176, 176, 1) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_176_train_orig_images.npy\"\n",
    "train_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_176_train_orig_labels.npy\"\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_labels.npy\"\n",
    "\n",
    "\n",
    "train_images, train_labels = load_images_and_labels(train_data)\n",
    "test_images, test_labels = load_images_and_labels(test_data)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#\n",
    "# Loss functions\n",
    "#\n",
    "#########################################\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_dice_coeff(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = 1 - weighted_dice_coeff(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "                                          (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + (1 - weighted_dice_coeff(y_true, y_pred, weight))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#\n",
    "# Unet models\n",
    "#\n",
    "################################\n",
    "\n",
    "class myUnet(object):\n",
    "\n",
    "    def __init__(self, image_size = 256, model_type = \"small\"):\n",
    "        self.img_rows = image_size\n",
    "        self.img_cols = image_size\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"small\":\n",
    "            self.build_unet_small()\n",
    "        elif model_type == \"large\":\n",
    "            self.build_unet()\n",
    "        elif model_type == \"large2\":\n",
    "            self.build_unet()\n",
    "        else :\n",
    "            print (\"Specify valid model_type (small, large, large2)\")\n",
    "            return\n",
    "\n",
    "    def load_data(self, train_data, test_data):\n",
    "        print('-'*30)\n",
    "        print(\"loading data\")\n",
    "        self.train_images, self.train_labels = load_images_and_labels(train_data)\n",
    "        print (\"Reduced size\", self.train_images.shape, self.train_labels.shape)\n",
    "        self.test_images, self.test_labels = load_images_and_labels(test_data)       \n",
    "        print(\"loading data done\")\n",
    "        print('-'*30)\n",
    "\n",
    "    def build_unet_small(self):\n",
    "        \n",
    "        '''\n",
    "        Input shape\n",
    "        4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' \n",
    "        or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last' (default format).\n",
    "        \n",
    "        Output shape\n",
    "        4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or \n",
    "        4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. \n",
    "        rows and cols values might have changed due to padding.\n",
    "        \n",
    "        He_normal initialization: It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) \n",
    "        where  fan_in is the number of input units in the weight tensor.\n",
    "        '''\n",
    "\n",
    "        print('-'*30)\n",
    "        print (\"Building smaller version of U-net model\")\n",
    "        print('-'*30)\n",
    "        \n",
    "        inputs = Input((self.img_rows, self.img_cols,1))\n",
    "\n",
    "        conv1 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        conv1 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        print (\"pool1 shape:\",pool1.shape)\n",
    "\n",
    "        conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool2 shape:\",pool2.shape)\n",
    "\n",
    "        conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        print (\"pool3 shape:\",pool3.shape)\n",
    "\n",
    "        conv4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        drop4 = Dropout(0.2)(conv4)\n",
    "        \n",
    "        up5 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n",
    "        merge5 = concatenate([conv3,up5], axis = 3)\n",
    "        conv5 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge5)\n",
    "        conv5 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        \n",
    "        up6 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv5))\n",
    "        merge6 = concatenate([conv2,up6], axis = 3)\n",
    "        conv6 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "        up7 = Conv2D(8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        merge7 = concatenate([conv1,up7], axis = 3)\n",
    "        conv7 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "        conv8 = Conv2D(1, 1, activation = 'sigmoid')(conv7)\n",
    "\n",
    "        self.model = Model(input = inputs, output = conv8)\n",
    "\n",
    "        self.model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        #model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "    def build_unet(self):\n",
    "        \n",
    "        '''\n",
    "        Input shape\n",
    "        4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' \n",
    "        or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last' (default format).\n",
    "        \n",
    "        Output shape\n",
    "        4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or \n",
    "        4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. \n",
    "        rows and cols values might have changed due to padding.\n",
    "        '''\n",
    "        print('-'*30)\n",
    "        print (\"Building U-net model\")\n",
    "        print('-'*30)\n",
    "        \n",
    "        inputs = Input((self.img_rows, self.img_cols,1))\n",
    "\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        print (\"pool1 shape:\",pool1.shape)\n",
    "\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool2 shape:\",pool2.shape)\n",
    "\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        print (\"pool3 shape:\",pool3.shape)\n",
    "\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        #drop4 = Dropout(0.5)(conv4)\n",
    "        drop4 = conv4\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        drop5 = Dropout(0.2)(conv5)\n",
    "\n",
    "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        merge6 = concatenate([drop4,up6], axis = 3)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        merge7 = concatenate([conv3,up7], axis = 3)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        merge8 = concatenate([conv2,up8], axis = 3)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        merge9 = concatenate([conv1,up9], axis = 3)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "        self.model = Model(input = inputs, output = conv10)\n",
    "\n",
    "        #self.model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        #self.model = multi_gpu_model(self.model, gpus=4)\n",
    "        self.model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def train_and_predict(self, model_file, batch_size = 4, nb_epoch = 10): \n",
    "        self.model_file = model_file #path to save the weights with best model\n",
    "        model_checkpoint = ModelCheckpoint(self.model_file, monitor='loss',verbose=1, save_best_only=True)\n",
    "        print('-'*30)\n",
    "        print('Fitting model...')\n",
    "        print('-'*30)\n",
    "        self.history = self.model.fit(self.train_images, self.train_labels, batch_size, nb_epoch, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "        print('-'*30)\n",
    "        print('predict test data')\n",
    "        self.predictions = self.model.predict(self.test_images, batch_size=1, verbose=1)\n",
    "        scores = self.model.evaluate (self.predictions, self.test_labels, batch_size=4)\n",
    "        print (\"Prediction Scores\", scores)\n",
    "        pred_file = \"predictions_sk2.npy\"\n",
    "        pred_file = UNET_TRAIN_DIR + pred_file\n",
    "        np.save(pred_file, self.predictions)\n",
    "        print('-'*30)\n",
    "        \n",
    "    \n",
    "    def train_with_augmentation(self, model_file, batch_size = 4, nb_epoch = 10 ):\n",
    "\n",
    "        sample_size, x_val, y_val, ax = self.train_images.shape\n",
    "\n",
    "        self.model_file = model_file #path to save the weights with best model\n",
    "        model_checkpoint = ModelCheckpoint(self.model_file, monitor='loss',verbose=1, save_best_only=True)\n",
    "        \n",
    "        # we create two instances with the same arguments\n",
    "        data_gen_args = dict(\n",
    "                             rotation_range=90.,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2)\n",
    "        \n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "        # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "        seed = 1\n",
    "        image_generator = image_datagen.flow(self.train_images, y=None, seed = seed, batch_size=sample_size)\n",
    "        mask_generator = mask_datagen.flow(self.train_labels,  y=None, seed = seed, batch_size=sample_size)\n",
    "        train_generator = zip(image_generator, mask_generator)\n",
    "        \n",
    "        print('-'*30)\n",
    "        print('Fitting model...')\n",
    "        \n",
    "        self.history = self.model.fit(self.train_images, self.train_labels, batch_size, nb_epoch, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "        \n",
    "        MAX_AUG=2\n",
    "        augmentation_round = 0\n",
    "        for img_tr, mask_tr in train_generator:\n",
    "                print (\"Augmentation round: \", augmentation_round+1, img_tr.shape)\n",
    "                s, x1, y1, p = img_tr.shape\n",
    "                self.history = self.model.fit(img_tr, mask_tr, batch_size, nb_epoch, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "                augmentation_round += 1\n",
    "                if (augmentation_round == MAX_AUG):\n",
    "                      break\n",
    "            \n",
    "        \n",
    "        print('-'*30)\n",
    "        print('Run Predictions on test data')\n",
    "        self.predictions = self.model.predict(self.test_images, batch_size=1, verbose=1)\n",
    "        scores = self.model.evaluate (self.predictions, self.test_labels, batch_size=4)\n",
    "        print (\"Prediction Scores\", scores)\n",
    "        pred_file = \"predictions_aug_sk1.npy\"\n",
    "        pred_file = UNET_TRAIN_DIR + pred_file\n",
    "        np.save(pred_file, self.predictions)\n",
    "        print('-'*30)\n",
    "        \n",
    "\n",
    "    def train_with_augmentation2(self, model_file, batch_size = 4, nb_epoch = 10 ):\n",
    "\n",
    "        sample_size, x_val, y_val, ax = self.train_images.shape\n",
    "\n",
    "        model_file = UNET_MODEL_DIR+'unet_aug2.hdf5' #path to save the weights with best model\n",
    "        model_checkpoint = ModelCheckpoint(model_file, monitor='loss',verbose=1, save_best_only=True)\n",
    "        \n",
    "        # we create two instances with the same arguments\n",
    "        data_gen_args = dict(featurewise_center=True,\n",
    "                             featurewise_std_normalization=True,\n",
    "                             rotation_range=90.,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2)\n",
    "        \n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "        # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "        seed = 1\n",
    "        image_datagen.fit(self.train_images, augment=True, seed=seed)\n",
    "        mask_datagen.fit(self.train_labels, augment=True, seed=seed)\n",
    "\n",
    "        # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "        seed = 1\n",
    "        image_generator = image_datagen.flow(self.train_images, y=None, seed = seed, batch_size=sample_size)\n",
    "        mask_generator = mask_datagen.flow(self.train_labels,  y=None, seed = seed, batch_size=sample_size)\n",
    "        train_generator = zip(image_generator, mask_generator)\n",
    "        \n",
    "        print('-'*30)\n",
    "        print('Fitting model...')\n",
    "        \n",
    "        self.model.fit(self.train_images, self.train_labels, batch_size, nb_epoch, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "        \n",
    "        MAX_AUG=2\n",
    "        augmentation_round = 0\n",
    "        for img_tr, mask_tr in train_generator:\n",
    "                print (\"Augmentation round: \", augmentation_round+1, img_tr.shape)\n",
    "                s, x1, y1, p = img_tr.shape\n",
    "                self.model.fit(img_tr, mask_tr, batch_size, nb_epoch, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])\n",
    "                augmentation_round += 1\n",
    "                if (augmentation_round == MAX_AUG):\n",
    "                      break\n",
    "            \n",
    "        \n",
    "        print('-'*30)\n",
    "        print('Run Predictions on test data')\n",
    "        self.predictions = self.model.predict(self.test_images, batch_size=1, verbose=1)\n",
    "        \n",
    "        pred_file = \"predictions_aug_sk1.npy\"\n",
    "        pred_file = UNET_TRAIN_DIR + pred_file\n",
    "        np.save(pred_file, self.predictions)\n",
    "        print('-'*30)\n",
    "        \n",
    "\n",
    "    def save_img(self):\n",
    "        pred_file = \"predictions.npy\"\n",
    "        pred_file = UNET_TRAIN_DIR + pred_file\n",
    "        print(\"array to image\")\n",
    "        imgs = np.load(pred_file)\n",
    "        for i in range(imgs.shape[0]):\n",
    "            img = imgs[i]\n",
    "            img = array_to_img(img)\n",
    "            img.save(\"./%d.jpg\"%(i))\n",
    "    \n",
    "    def plot_accuracy_and_loss(self):\n",
    "        # list all data in history\n",
    "        print(self.history.history.keys())\n",
    "        history = myunet.history\n",
    "        # summarize history for accuracy\n",
    "        if 'dice_coeff' in self.history.history.keys():\n",
    "            plt.plot(history.history['dice_coeff'])\n",
    "            plt.plot(history.history['val_dice_coeff'])\n",
    "            plt.title('model accuracy(dice_coeff)')\n",
    "        elif 'val_acc' in self.history.history.keys():\n",
    "            plt.plot(history.history['acc'])\n",
    "            plt.plot(history.history['val_acc'])\n",
    "            plt.title('model accuracy')\n",
    "        else : \n",
    "            print (\"new loss function, not in the list\")\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Building U-net model\n",
      "------------------------------\n",
      "conv1 shape: (?, 176, 176, 64)\n",
      "conv1 shape: (?, 176, 176, 64)\n",
      "pool1 shape: (?, 88, 88, 64)\n",
      "conv2 shape: (?, 88, 88, 128)\n",
      "conv2 shape: (?, 88, 88, 128)\n",
      "pool2 shape: (?, 44, 44, 128)\n",
      "conv3 shape: (?, 44, 44, 256)\n",
      "conv3 shape: (?, 44, 44, 256)\n",
      "pool3 shape: (?, 22, 22, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:173: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "loading data\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /opt/output/acdc/unet_model/data/acdc_176_train_orig_images.npy /opt/output/acdc/unet_model/data/acdc_176_train_orig_labels.npy\n",
      "max :  2952.0\n",
      "(1657, 176, 176, 1) 1.0 0.0\n",
      "(1657, 176, 176, 1) 1.0 0.0\n",
      "Reduced size (1657, 176, 176, 1) (1657, 176, 176, 1)\n",
      "------------------------------\n",
      "load np arrays of images and labels...\n",
      "------------------------------\n",
      "Loading files :  /opt/output/acdc/unet_model/data/acdc_176_test_orig_images.npy /opt/output/acdc/unet_model/data/acdc_176_test_orig_labels.npy\n",
      "max :  1814.0\n",
      "(184, 176, 176, 1) 1.0 0.0\n",
      "(184, 176, 176, 1) 1.0 0.0\n",
      "loading data done\n",
      "------------------------------\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 176, 176, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 176, 176, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 176, 176, 64) 36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 88, 88, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 88, 88, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 88, 88, 128)  147584      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 44, 44, 128)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 44, 44, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 44, 44, 256)  590080      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 22, 22, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 22, 22, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 22, 22, 512)  2359808     conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 11, 11, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 11, 11, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 11, 11, 1024) 9438208     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 11, 11, 1024) 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 22, 22, 1024) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 22, 22, 512)  2097664     up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 22, 22, 1024) 0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 22, 22, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 22, 22, 512)  2359808     conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 44, 44, 512)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 44, 44, 256)  524544      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 44, 44, 512)  0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 44, 44, 256)  1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 44, 44, 256)  590080      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 88, 88, 256)  0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 88, 88, 128)  131200      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 88, 88, 256)  0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 88, 88, 128)  295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 88, 88, 128)  147584      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 176, 176, 128 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 176, 176, 64) 32832       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 176, 176, 128 0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 176, 176, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 176, 176, 64) 36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 176, 176, 1)  65          conv2d_22[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Create a U-Net model, train the model and run the predictions and save the trained weights and predictions\n",
    "############\n",
    "train_data = {}\n",
    "train_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_176_train_orig_images.npy\"\n",
    "train_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_176_train_orig_labels.npy\"\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_labels.npy\"\n",
    "\n",
    "model_file = UNET_MODEL_DIR+'unet_large_176.hdf5'\n",
    "\n",
    "myunet = myUnet(image_size = 176, model_type = \"large\")\n",
    "\n",
    "myunet.load_data(train_data, test_data)\n",
    "\n",
    "myunet.model.summary()\n",
    "\n",
    "#with tf.device(\"GPU:1\"):\n",
    "#    print (tf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 1325 samples, validate on 332 samples\n",
      "Epoch 1/20\n",
      "1325/1325 [==============================] - 49s 37ms/step - loss: 0.1214 - acc: 0.9579 - val_loss: 0.0647 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.12140, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 2/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0550 - acc: 0.9804 - val_loss: 0.0524 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00002: loss improved from 0.12140 to 0.05496, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 3/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0331 - acc: 0.9880 - val_loss: 0.0212 - val_acc: 0.9919\n",
      "\n",
      "Epoch 00003: loss improved from 0.05496 to 0.03313, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 4/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0223 - acc: 0.9917 - val_loss: 0.0165 - val_acc: 0.9933\n",
      "\n",
      "Epoch 00004: loss improved from 0.03313 to 0.02226, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 5/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0171 - acc: 0.9933 - val_loss: 0.0225 - val_acc: 0.9909\n",
      "\n",
      "Epoch 00005: loss improved from 0.02226 to 0.01709, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 6/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0172 - acc: 0.9934 - val_loss: 0.0158 - val_acc: 0.9937\n",
      "\n",
      "Epoch 00006: loss did not improve\n",
      "Epoch 7/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0125 - acc: 0.9949 - val_loss: 0.0119 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00007: loss improved from 0.01709 to 0.01252, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 8/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0108 - acc: 0.9955 - val_loss: 0.0118 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00008: loss improved from 0.01252 to 0.01081, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 9/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0101 - acc: 0.9957 - val_loss: 0.0117 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00009: loss improved from 0.01081 to 0.01014, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 10/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0099 - acc: 0.9958 - val_loss: 0.0136 - val_acc: 0.9945\n",
      "\n",
      "Epoch 00010: loss improved from 0.01014 to 0.00990, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 11/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0251 - val_acc: 0.9899\n",
      "\n",
      "Epoch 00011: loss did not improve\n",
      "Epoch 12/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0125 - acc: 0.9951 - val_loss: 0.0106 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00012: loss did not improve\n",
      "Epoch 13/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0094 - acc: 0.9960 - val_loss: 0.0100 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00013: loss improved from 0.00990 to 0.00942, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 14/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0088 - acc: 0.9962 - val_loss: 0.0105 - val_acc: 0.9956\n",
      "\n",
      "Epoch 00014: loss improved from 0.00942 to 0.00882, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 15/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0086 - acc: 0.9963 - val_loss: 0.0108 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00015: loss improved from 0.00882 to 0.00857, saving model to /opt/output/acdc/unet_model/models/unet_large_176.hdf5\n",
      "Epoch 16/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0088 - acc: 0.9963 - val_loss: 0.0151 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00016: loss did not improve\n",
      "Epoch 17/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0097 - acc: 0.9960 - val_loss: 0.0198 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00017: loss did not improve\n",
      "Epoch 18/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0166 - acc: 0.9942 - val_loss: 0.0119 - val_acc: 0.9951\n",
      "\n",
      "Epoch 00018: loss did not improve\n",
      "Epoch 19/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0120 - acc: 0.9952 - val_loss: 0.0115 - val_acc: 0.9954\n",
      "\n",
      "Epoch 00019: loss did not improve\n",
      "Epoch 20/20\n",
      "1325/1325 [==============================] - 41s 31ms/step - loss: 0.0097 - acc: 0.9959 - val_loss: 0.0104 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00020: loss did not improve\n",
      "------------------------------\n",
      "predict test data\n",
      "184/184 [==============================] - 2s 13ms/step\n",
      "184/184 [==============================] - 1s 8ms/step\n",
      "Prediction Scores [0.21271381624366925, 0.9842093755369601]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#print(sess.run(myunet.train_and_predict(model_file, batch_size = 2, nb_epoch = 10)))\n",
    "        \n",
    "res = myunet.train_and_predict(model_file, batch_size = 4, nb_epoch = 20)\n",
    "#res = myunet.train_with_augmentation(model, batch_size = 4, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss', 'acc', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f308d1f09b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3084549828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myunet.plot_accuracy_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Method to visually compare the predictions with actual labels\n",
    "#\n",
    "################################\n",
    "\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_176_test_orig_labels.npy\"\n",
    "\n",
    "ts , tl= load_images_and_labels(test_data)\n",
    "\n",
    "pred = myunet.predictions\n",
    "samples, x, y, z = pred.shape\n",
    "pred2 = np.round(pred)\n",
    "print (samples, pred.max(), pred.min())\n",
    "scores = myunet.model.evaluate (pred2, tl, batch_size=4)\n",
    "print (\"Prediction Scores\", scores)\n",
    "\n",
    "##Print few images wih actual labels and predictions\n",
    "for i in range (8,16):\n",
    "    f, axs = plt.subplots(1,3,figsize=(15,15))\n",
    "    plt.subplot(131),plt.imshow(ts[i].reshape(x, y))\n",
    "    plt.title('test Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(132),plt.imshow(tl[i].reshape(x, y))\n",
    "    plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(133),plt.imshow(pred2[i].reshape(x, y))\n",
    "    plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "#     plt.subplot(132),plt.imshow(ts[i].reshape(256, 256)), plt.imshow(tl[i].reshape(256, 256), 'jet', interpolation='none', alpha=0.5)\n",
    "#     plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "#     plt.subplot(133),plt.imshow(ts[i].reshape(256, 256)), plt.imshow(pred[i].reshape(256, 256), 'jet', interpolation='none', alpha=0.5)\n",
    "#     plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# img = imgs[5].reshape(256,256)\n",
    "\n",
    "# plt.imshow(img, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = myunet.predictions\n",
    "scores = myunet.model.evaluate (pred, tl, batch_size=4)\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = pred\n",
    "p2[(p2>0.99)] = 1\n",
    "p2[(p2<=0.99)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = myunet.model.evaluate (p2, tl, batch_size=1)\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Create a U-Net model, train the model and run the predictions and save the trained weights and predictions\n",
    "############\n",
    "# from numpy.random import seed\n",
    "# seed(123)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(200)\n",
    "\n",
    "train_data = {}\n",
    "train_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_256_train_orig_images.npy\"\n",
    "train_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_256_train_orig_labels.npy\"\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_labels.npy\"\n",
    "\n",
    "model_file = UNET_MODEL_DIR+'unet_256.hdf5'\n",
    "\n",
    "myunet2 = myUnet(image_size = 256, model_type = \"large\")\n",
    "#model = myunet.get_unet_small()\n",
    "\n",
    "myunet2.model.summary()\n",
    "myunet2.load_data (train_data, test_data)\n",
    "res = myunet2.train_and_predict(model_file, batch_size = 4, nb_epoch = 10)\n",
    "\n",
    "#res = myunet.train_with_augmentation(model, batch_size = 4, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myunet2.plot_accuracy_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "#\n",
    "# Method to visually compare the predictions with actual labels\n",
    "#\n",
    "################################\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_labels.npy\"\n",
    "\n",
    "ts , tl= load_images_and_labels(test_data)\n",
    "\n",
    "pred = myunet2.predictions\n",
    "samples, x, y, z = pred.shape\n",
    "print (samples, pred.max(), pred.min())\n",
    "\n",
    "scores = myunet2.model.evaluate (pred, tl, batch_size=4)\n",
    "print (\"Prediction Scores\", scores)\n",
    "\n",
    "##Print few images wih actual labels and predictions\n",
    "for i in range (8,16):\n",
    "    f, axs = plt.subplots(1,3,figsize=(10,10))\n",
    "    plt.subplot(131),plt.imshow(ts[i].reshape(x, y))\n",
    "    plt.title('test Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(132),plt.imshow(tl[i].reshape(x, y))\n",
    "    plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(133),plt.imshow(pred[i].reshape(x, y))\n",
    "    plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "#     plt.subplot(132),plt.imshow(ts[i].reshape(256, 256)), plt.imshow(tl[i].reshape(256, 256), 'jet', interpolation='none', alpha=0.5)\n",
    "#     plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "#     plt.subplot(133),plt.imshow(ts[i].reshape(256, 256)), plt.imshow(pred[i].reshape(256, 256), 'jet', interpolation='none', alpha=0.5)\n",
    "#     plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "#\n",
    "# Method to load the trained weights into U-net model and run the predictions\n",
    "#\n",
    "########################################\n",
    "\n",
    "test_data = {}\n",
    "test_data[\"images\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_images.npy\"\n",
    "test_data[\"labels\"] = UNET_TRAIN_DIR + SOURCE + \"_256_test_orig_labels.npy\"\n",
    "\n",
    "model_file = UNET_MODEL_DIR+'unet_256.hdf5' #path to save the weights with best model\n",
    "\n",
    "print('-'*30)\n",
    "print (\"Loading Test images and labels...\")\n",
    "ts , tl= load_images_and_labels(test_data)\n",
    "\n",
    "print('-'*30)\n",
    "print (\"Creating U-net model...\")\n",
    "myunet256 = myUnet(image_size = 256, model_type = \"large\")\n",
    "print('-'*30)\n",
    "print (\"Loading the pre-trained weights...\")\n",
    "myunet256.model.load_weights(model_file)\n",
    "print('-'*30)\n",
    "\n",
    "print('Run predictions...')\n",
    "pred = myunet256.model.predict(ts, batch_size=1, verbose=1)\n",
    "samples, x, y, z = pred.shape\n",
    "print (\"Pred data :\", pred.shape, pred.max(), pred.min())\n",
    "print('-'*30)\n",
    "\n",
    "\n",
    "\n",
    "for i in range (10,15):\n",
    "    f, axs = plt.subplots(1,3,figsize=(12,12))\n",
    "    plt.subplot(131),plt.imshow(ts[i].reshape(x, y))\n",
    "    plt.title('test Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(132),plt.imshow(ts[i].reshape(x, y)), plt.imshow(tl[i].reshape(x, y), 'jet', interpolation='none', alpha=0.5)\n",
    "    plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(133),plt.imshow(ts[i].reshape(x, y)), plt.imshow(pred[i].reshape(x, y), 'jet', interpolation='none', alpha=0.5)\n",
    "    plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ............ Backup section  (test code)...................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.where(np.logical_and(pred>0, pred<1))\n",
    "b = pred[(pred>0.5) & (pred<1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(myunet2.history.history.keys())\n",
    "history = myunet2.history\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(b)), b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one():\n",
    "    image_batch, mask_batch = next(validation_generator)\n",
    "    predicted_mask_batch = model.predict(image_batch)\n",
    "    image = image_batch[0]\n",
    "    predicted_mask = predicted_mask_batch[0].reshape(SIZE)\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(predicted_mask, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myunet = myUnet()\n",
    "model = myunet.get_unet()\n",
    "model.summary()\n",
    "\n",
    "res = myunet.train_and_predict(model, batch_size = 4, nb_epoch = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = load_test_data()\n",
    "tl = load_test_labels()\n",
    "\n",
    "\n",
    "myunet = myUnet()\n",
    "model = myunet.get_unet()\n",
    "#model.load_weights('unet.hdf5')\n",
    "model.load_weights('unet_small.hdf5')\n",
    "print('predict test data')\n",
    "pred = model.predict(ts, batch_size=1, verbose=1)\n",
    "\n",
    "print (pred.max(), pred.min())\n",
    "\n",
    "for i in range (4,8):\n",
    "    plt.subplot(131),plt.imshow(ts[i].reshape(256, 256))\n",
    "    plt.title('test Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(132),plt.imshow(tl[i].reshape(256, 256))\n",
    "    plt.title('test label'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(133),plt.imshow(pred[i].reshape(256, 256))\n",
    "    plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pred.max(), pred.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myunet = myUnet()\n",
    "model = myunet.get_unet2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_img = ts[4].reshape(256, 256)\n",
    "clahe = cv2.createCLAHE(tileGridSize=(1, 1))\n",
    "cl_img = clahe.apply(sq_img)\n",
    "plt.imshow(pred[i].reshape(256, 256))\n",
    "plt.title('Predicted mask'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
